package org.myorg;

import java.io.IOException;
import java.net.URI;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import java.lang.String;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import java.util.regex.Pattern;


public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	
public void map(LongWritable key, Text value, Context context)
throws IOException, InterruptedException
{

        // Read the input filename into the mapper, calling it 'TitleScene'
     String TitleScene = ((FileSplit) context.getInputSplit()).getPath().getName();

   
        // Input files are in the form of BOOKTITLE_CHAPTER.txt, ie PridePrejudice_15.txt
	// The code below emits a String with the Book Title, Chapter and Emotion
	// In this example the cached emotion is Joy
    Pattern pattern = Pattern.compile("_");
    String[] split = pattern.split(TitleScene.substring(0, TitleScene.length()-3));
	String TitleSceneEmotion = split[0] + ", " + split[1] + ", " + "Joy";
    
  		// Set up the mapper to access the file from the Distributed Cache  
    URI[] cacheFile = context.getCacheFiles();

    
   		// Break line into words for processing and convert all letters to lower case
    StringTokenizer wordList = new StringTokenizer(value.toString().toLowerCase);
		String currentToken = null;
		while (wordList.hasMoreTokens()) {
			currentToken = wordList.nextToken();
		// if the current word is present in the cachefile then generate a key-value pair
			if (cacheFile.contains(currentToken)) {
			
        		    context.write(new Text(TitleSceneEmotion), new IntWritable(1));
			
            }
            
    }        
            
    }
}
