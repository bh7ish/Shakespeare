package org.myorg;
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import java.lang.String;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import java.util.regex.Pattern;


public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	
public void map(LongWritable key, Text value, Context context)
throws IOException, InterruptedException
{

        // Read the input filename into the mapper, calling it 'Title'
	String Title = ((FileSplit) context.getInputSplit()).getPath().getName();
   
        // Input files are in the form of BOOKTITLE_CHAPTER.txt, ie PridePrejudice_15.txt
		// The code below emits a String with The Book Title, Chapter and Emotion
		// In this example the cached file is Joy
    Pattern pattern = Pattern.compile("_");
    String[] split = pattern.split(Title.substring(0, Title.length()-3));
	String TitleSceneEmotion = split[0] + ", " + split[1] + ", " + "Joy";
    
  		// Set up the mapper to access the file from the Distributed Cache  
    URI[] cacheFile = context.getCacheFiles();

    
   		// Break line into words for processing
    StringTokenizer wordList = new StringTokenizer(value.toString());
		String currentToken = null;
		while (wordList.hasMoreTokens()) {
			currentToken = wordList.nextToken();
		// if the current word is present in the cachefile then generate a key-value pair
			if (cacheFile.contains(currentToken)) {
			
        		    context.write(new Text(TitleSceneEmotion), new IntWritable(1));
			
            }
            
    }        
            
    }
