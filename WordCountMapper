package org.myorg;
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashSet;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>
{



@Override
public void map(LongWritable key, Text value, Context context)
throws IOException, InterruptedException
{
	
	 URI[] cacheFile = context.getCacheFiles();
	

	    HashSet<String> textList = new HashSet<String>();
		
		String filename=null;
		int lastindex =cacheFile[0].toString().lastIndexOf('/');
		if(lastindex!=-1)
		{
			filename =cacheFile[0].toString().substring(lastindex+1, cacheFile[0].toString().length());
			
		}
		else
		{
			filename=cacheFile[0].toString();
		}
		

		
		
		BufferedReader reader = new BufferedReader(new FileReader(filename));
		String line = null;
		while ((line = reader.readLine()) != null) {
			System.out.println(line+"  ");
			StringTokenizer listTokens = new StringTokenizer(line);
			while(listTokens.hasMoreTokens())
			{	
				
				textList.add(listTokens.nextToken());
			}
			
		}
		reader.close();
		
		//Read the text files present in input folder
		StringTokenizer  textTokens = new StringTokenizer(value.toString());
		
		while (textTokens.hasMoreTokens()) {
				int count=1;
				String word= textTokens.nextToken();
				
			
				if(textList.contains(word))
				{
					context.write(new Text(word), new IntWritable(count));
				}
		}
	}
	
}

